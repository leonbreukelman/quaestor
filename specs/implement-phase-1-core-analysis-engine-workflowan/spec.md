# Feature Specification: Phase 1 Core Analysis Engine

**Generated**: 2026-01-13T22:49:33.069135
**Status**: Draft
**Source**: Auto-generated by Smactorio workflow

---

## Overview

Implement the foundational analysis engine for agent code inspection, consisting of a WorkflowAnalyzer DSPy module for semantic analysis, a Python AST parser built on tree-sitter for structural code parsing, and a static linter for automated code quality checks. This engine forms the core capability for analyzing and validating agent workflows and code patterns.

## Problem Statement

Agent code and workflows lack automated analysis tooling to detect issues, validate patterns, and ensure quality. Developers need a robust analysis engine that can parse Python code structurally, analyze agent workflows semantically, and provide static linting feedback to catch problems early in the development cycle.

---

## User Stories

### US1 - Parse Python Agent Code with Tree-sitter (Priority: P0)

As a **developer**, I want to **parse Python agent code files using tree-sitter to extract AST representations**, so that **I can programmatically analyze code structure, identify patterns, and extract relevant code elements for further analysis**.

**Independent Test**: Given a sample Python file containing an agent class with multiple methods, when parsed through the tree-sitter parser, then the output AST contains all class definitions, method definitions, and their corresponding line numbers.

**Acceptance Criteria**:

1. Parser accepts valid Python source code as input
2. Parser returns a complete AST representation of the code
3. Parser handles syntax errors gracefully with meaningful error messages
4. Parser supports Python 3.8+ syntax features
5. Parser extracts function definitions, class definitions, and import statements

### US2 - Analyze Agent Workflows with DSPy Module (Priority: P0)

As a **developer**, I want to **analyze agent workflow patterns using the WorkflowAnalyzer DSPy module**, so that **I can understand the semantic structure of agent workflows and identify potential issues or optimization opportunities**.

**Independent Test**: Given an agent code file with a defined workflow containing sequential steps and a conditional branch, when analyzed by WorkflowAnalyzer, then the output correctly identifies the workflow structure including all steps and the branching condition.

**Acceptance Criteria**:

1. WorkflowAnalyzer accepts parsed AST as input
2. WorkflowAnalyzer identifies agent workflow entry points
3. WorkflowAnalyzer traces execution paths through agent code
4. WorkflowAnalyzer detects common workflow patterns (sequential, branching, looping)
5. WorkflowAnalyzer outputs structured analysis results in a defined schema

### US3 - Run Static Linting on Agent Code (Priority: P0)

As a **developer**, I want to **run static linting checks on my agent code to detect common issues and anti-patterns**, so that **I can catch potential bugs, style violations, and anti-patterns before runtime**.

**Independent Test**: Given an agent code file with intentional issues (unused import, undefined variable reference, missing docstring), when the linter runs, then it reports all issues with correct severity, location, and descriptive messages.

**Acceptance Criteria**:

1. Linter analyzes Python agent code files
2. Linter reports issues with severity levels (error, warning, info)
3. Linter provides file path, line number, and column for each issue
4. Linter includes descriptive messages explaining each issue
5. Linter supports configurable rule sets
6. Linter defaults to non-destructive read-only analysis

### US4 - Integrate Analysis Components (Priority: P1)

As a **developer**, I want to **use a unified API to run the complete analysis pipeline (parse, analyze, lint) on agent code**, so that **I can perform comprehensive code analysis with a single API call without managing individual components**.

**Independent Test**: Given a valid agent code file path, when calling the unified analysis API, then the response contains AST data, workflow analysis results, and linting findings in a single structured response.

**Acceptance Criteria**:

1. Unified API accepts file path or source code string as input
2. API orchestrates parsing, workflow analysis, and linting in sequence
3. API returns combined results from all analysis stages
4. API handles failures in individual stages gracefully
5. API provides clear error messages when analysis cannot proceed

### US5 - Configure Linter Rules (Priority: P2)

As a **developer**, I want to **configure which linting rules are enabled or disabled for my project**, so that **I can customize the analysis to match my project's coding standards and ignore rules that don't apply**.

**Independent Test**: Given a configuration file that disables the 'missing-docstring' rule, when running the linter on code with missing docstrings, then no 'missing-docstring' violations are reported.

**Acceptance Criteria**:

1. Configuration accepts a list of rule IDs to enable or disable
2. Configuration can be provided via file or programmatic API
3. Invalid rule IDs produce clear warning messages
4. Default configuration enables all standard rules
5. Configuration changes do not require code changes

### US6 - Export Analysis Results (Priority: P2)

As a **API consumer**, I want to **export analysis results in JSON format for integration with other tools**, so that **I can integrate the analysis engine with CI/CD pipelines, IDEs, and other development tools**.

**Independent Test**: Given completed analysis results, when exporting to JSON, then the output is valid JSON that conforms to the documented schema and can be parsed by standard JSON libraries.

**Acceptance Criteria**:

1. Results can be exported as valid JSON
2. JSON schema is documented and stable
3. Export includes all analysis data (AST, workflow, lint results)
4. Export includes metadata (timestamp, version, input file)
5. Large result sets are handled efficiently

---

## Entities

### ASTNode
**Type**: entity
**Description**: Represents a node in the parsed abstract syntax tree
**Attributes**:
  - type
  - start_position
  - end_position
  - children
  - text

### WorkflowAnalysis
**Type**: entity
**Description**: Results of semantic workflow analysis
**Attributes**:
  - entry_points
  - execution_paths
  - patterns
  - complexity_score

### LintResult
**Type**: entity
**Description**: Individual linting finding
**Attributes**:
  - rule_id
  - severity
  - message
  - file_path
  - line
  - column

### AnalysisReport
**Type**: entity
**Description**: Combined output from all analysis stages
**Attributes**:
  - ast
  - workflow_analysis
  - lint_results
  - metadata

### LinterConfiguration
**Type**: entity
**Description**: Configuration for linting rules
**Attributes**:
  - enabled_rules
  - disabled_rules
  - severity_overrides

## Functional Requirements

FR-001: FR-001: The system SHALL parse Python source code using tree-sitter and produce an AST representation
FR-002: FR-002: The system SHALL support Python 3.8+ syntax in the parser
FR-003: FR-003: The WorkflowAnalyzer SHALL identify agent workflow entry points from parsed AST
FR-004: FR-004: The WorkflowAnalyzer SHALL trace and document execution paths through agent code
FR-005: FR-005: The WorkflowAnalyzer SHALL detect workflow patterns (sequential, branching, looping)
FR-006: FR-006: The static linter SHALL analyze Python agent code for common issues and anti-patterns
FR-007: FR-007: The linter SHALL report issues with severity level, location, and descriptive message
FR-008: FR-008: The linter SHALL support configurable rule sets
FR-009: FR-009: The system SHALL provide a unified API for running the complete analysis pipeline
FR-010: FR-010: The system SHALL export analysis results in JSON format

## Non-Functional Requirements

NFR-001: NFR-001: The parser SHALL process files up to 10,000 lines within 5 seconds
NFR-002: NFR-002: The analysis engine SHALL be thread-safe for concurrent analysis requests
NFR-003: NFR-003: The system SHALL have test coverage of at least 80% for all public APIs
NFR-004: NFR-004: All analysis operations SHALL be non-destructive (read-only)
NFR-005: NFR-005: The JSON export schema SHALL be versioned and backward compatible

## Business Rules

- BR-001: All analysis operations must be read-only and non-destructive by default
- BR-002: Linter rules must have unique identifiers following the pattern 'category-name'
- BR-003: Analysis failures in one stage should not prevent other stages from completing
- BR-004: Default linter configuration must enable all standard rules

## Assumptions

- Tree-sitter Python grammar is available and maintained
- DSPy framework is available for building the WorkflowAnalyzer module
- Input code files are UTF-8 encoded
- Analysis targets single files initially, with batch processing in future phases

## Open Questions

- [ ] What specific agent workflow patterns should be detected in Phase 1?
- [ ] Should the linter support custom rule plugins in Phase 1 or defer to later phases?
- [ ] What is the expected maximum file size for analysis?
- [ ] Should the analysis engine support incremental parsing for large files?

## Success Criteria

- Tree-sitter parser successfully parses valid Python 3.8+ code and produces correct AST
- WorkflowAnalyzer correctly identifies workflow patterns in sample agent code
- Static linter detects at least 10 common code issues with accurate location reporting
- Unified API successfully orchestrates all analysis components
- All public APIs have contract tests written before implementation (TDD)
- Analysis operations complete without modifying input files (safety-by-default)
- JSON export produces valid, schema-compliant output
- Test coverage meets or exceeds 80% threshold
