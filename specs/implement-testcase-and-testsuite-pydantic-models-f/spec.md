# Feature Specification: TestCase and TestSuite Pydantic Models for Phase 2 Test Generation

**Generated**: 2026-01-14T12:04:46.584195
**Status**: Draft
**Source**: Auto-generated by Smactorio workflow

---

## Overview

Implement Pydantic models for test generation including TestCase, TestSuite, TestResult, and Assertion types. These models use discriminated unions for assertion types with type-specific validation, support both JSON and YAML serialization for OSCAL compatibility, and maintain loose coupling to analysis models through optional references.

## Problem Statement

Phase 2 Test Generation requires structured data models to represent test cases, test suites, and their results. Without well-defined models, test generation cannot produce consistent, serializable output that integrates with the existing analysis infrastructure and OSCAL compliance requirements.

---

## User Stories

### US1 - Define Assertion Types with Discriminated Union (Priority: P0)

As a **developer**, I want to **define multiple assertion types (equals, contains, regex, tool_called, state_reached, schema_valid) using a discriminated union pattern**, so that **I can create type-safe assertions with appropriate validation for each assertion type**.

**Independent Test**: Create instances of each assertion type and verify Pydantic validates type-specific fields correctly. Verify invalid data raises ValidationError.

**Acceptance Criteria**:

1. Assertion base class/type exists with 'type' discriminator field
2. EqualsAssertion validates expected value presence
3. ContainsAssertion validates substring value presence
4. RegexAssertion validates pattern is a valid regex
5. ToolCalledAssertion validates tool_name is non-empty
6. StateReachedAssertion validates state_name is non-empty
7. SchemaValidAssertion validates JSON schema is well-formed
8. Union type correctly discriminates based on 'type' field

### US2 - Create TestCase Model (Priority: P0)

As a **developer**, I want to **define a TestCase model with id, name, description, input, expected assertions, and optional target references**, so that **I can represent individual test cases with all necessary metadata and assertions**.

**Independent Test**: Create TestCase instances with various assertion combinations and verify all fields serialize/deserialize correctly.

**Acceptance Criteria**:

1. TestCase has required fields: id, name, input
2. TestCase has optional fields: description, target_tool, target_state
3. TestCase contains a list of Assertion objects
4. TestCase validates that at least one assertion is present
5. Optional target_tool and target_state maintain loose coupling to analysis models

### US3 - Create TestSuite Model (Priority: P0)

As a **developer**, I want to **define a TestSuite model that groups multiple TestCase instances with metadata**, so that **I can organize related test cases into logical suites for batch execution**.

**Independent Test**: Create TestSuite with multiple TestCases and verify collection operations work correctly.

**Acceptance Criteria**:

1. TestSuite has required fields: id, name, test_cases
2. TestSuite has optional fields: description, tags, created_at
3. TestSuite contains a list of TestCase objects
4. TestSuite validates that test_cases is non-empty
5. TestSuite supports adding/removing test cases

### US4 - Create TestResult Model (Priority: P0)

As a **developer**, I want to **define a TestResult model to capture test execution outcomes**, so that **I can record and analyze test execution results with detailed pass/fail information**.

**Independent Test**: Create TestResult instances for passing and failing scenarios, verify computed fields work correctly.

**Acceptance Criteria**:

1. TestResult has required fields: test_case_id, passed, executed_at
2. TestResult has optional fields: actual_output, error_message, duration_ms
3. TestResult includes assertion_results list showing individual assertion outcomes
4. TestResult passed field is computed from all assertion results

### US5 - JSON Serialization Support (Priority: P1)

As a **API consumer**, I want to **serialize and deserialize all models to/from JSON format**, so that **I can exchange test data with other systems using standard JSON format**.

**Independent Test**: Serialize complex TestSuite to JSON and deserialize back, verify equality.

**Acceptance Criteria**:

1. All models support model_dump_json() method
2. All models support model_validate_json() class method
3. Discriminated unions serialize with type field preserved
4. Round-trip serialization produces equivalent objects

### US6 - YAML Serialization Support (Priority: P1)

As a **OSCAL integrator**, I want to **serialize and deserialize all models to/from YAML format**, so that **I can integrate with OSCAL-compliant systems that prefer YAML format**.

**Independent Test**: Serialize TestSuite to YAML, verify human-readability, deserialize and verify equality.

**Acceptance Criteria**:

1. All models support to_yaml() method
2. All models support from_yaml() class method
3. YAML output is human-readable with proper indentation
4. Round-trip YAML serialization produces equivalent objects

### US7 - Follow Existing Model Patterns (Priority: P1)

As a **developer**, I want to **implement models following patterns established in analysis/models.py**, so that **I can maintain consistency across the codebase and reduce learning curve**.

**Independent Test**: Code review comparing new models against analysis/models.py patterns.

**Acceptance Criteria**:

1. Models use same base configuration as analysis models
2. Field naming conventions match existing patterns
3. Validation patterns are consistent with existing models
4. Import structure follows established conventions

### US8 - Achieve 100% Test Coverage (Priority: P1)

As a **QA engineer**, I want to **have comprehensive test coverage for all model classes and methods**, so that **I can be confident that all model behavior is verified and regressions are caught**.

**Independent Test**: Run pytest with coverage and verify 100% coverage for testing/models.py module.

**Acceptance Criteria**:

1. All model classes have corresponding test files
2. All public methods have at least one test
3. Edge cases for validation are tested
4. Coverage report shows 100% line coverage for models module

---

## Entities

### Assertion
**Type**: entity
**Description**: Base type for test assertions using discriminated union pattern

### EqualsAssertion
**Type**: entity
**Description**: Assertion that checks exact equality

### ContainsAssertion
**Type**: entity
**Description**: Assertion that checks substring presence

### RegexAssertion
**Type**: entity
**Description**: Assertion that checks regex pattern match

### ToolCalledAssertion
**Type**: entity
**Description**: Assertion that verifies a tool was called

### StateReachedAssertion
**Type**: entity
**Description**: Assertion that verifies a state was reached

### SchemaValidAssertion
**Type**: entity
**Description**: Assertion that validates output against JSON schema

### TestCase
**Type**: entity
**Description**: Individual test case with input, assertions, and optional references

### TestSuite
**Type**: entity
**Description**: Collection of related test cases

### TestResult
**Type**: entity
**Description**: Result of executing a test case

## Functional Requirements

FR-001: FR-001: Implement discriminated union Assertion type with 6 variants (equals, contains, regex, tool_called, state_reached, schema_valid)
FR-002: FR-002: Each assertion variant MUST have type-specific validation
FR-003: FR-003: TestCase model MUST support optional target_tool and target_state references
FR-004: FR-004: TestSuite model MUST contain non-empty list of TestCase objects
FR-005: FR-005: All models MUST support JSON serialization via Pydantic
FR-006: FR-006: All models MUST support YAML serialization for OSCAL compatibility
FR-007: FR-007: TestResult MUST track individual assertion outcomes
FR-008: FR-008: RegexAssertion MUST validate that pattern is a valid regex at construction time

## Non-Functional Requirements

NFR-001: NFR-001: Achieve 100% test coverage for all model classes
NFR-002: NFR-002: Follow patterns established in analysis/models.py
NFR-003: NFR-003: Maintain loose coupling between testing models and analysis models
NFR-004: NFR-004: YAML output MUST be human-readable with proper formatting

## Business Rules

- BR-001: TestCase MUST have at least one assertion
- BR-002: TestSuite MUST have at least one TestCase
- BR-003: TestResult.passed is computed from all assertion_results
- BR-004: Destructive operations on test data require explicit confirmation per SAFETY-BY-DEFAULT principle

## Assumptions

- Pydantic v2 is available and used throughout the project
- PyYAML or ruamel.yaml is available for YAML serialization
- analysis/models.py exists and provides reference patterns
- OSCAL compatibility requires standard YAML format without custom tags

## Open Questions

- [ ] Should TestResult include reference to full TestCase or just test_case_id?
- [ ] What is the expected format for tool arguments in ToolCalledAssertion?
- [ ] Should SchemaValidAssertion accept JSON Schema draft-07 or later versions?

## Success Criteria

- All 4 models (Assertion, TestCase, TestSuite, TestResult) are implemented and validated
- Discriminated union pattern correctly routes to appropriate assertion type
- 100% test coverage achieved as verified by pytest-cov
- JSON round-trip serialization works for all models
- YAML round-trip serialization works for all models
- Models follow patterns from analysis/models.py
- Optional target references maintain loose coupling
- All tests pass in CI pipeline
